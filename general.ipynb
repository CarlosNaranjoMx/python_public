{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CarlosNaranjoMx/python_public/blob/main/general.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatGpt"
      ],
      "metadata": {
        "id": "4CqnCB9ELEsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install x11-utils > /dev/null 2>&1\n",
        "!pip install pyglet > /dev/null 2>&1\n",
        "!pip install openai > /dev/null 2>&1\n",
        "!pip install pyperclip > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "hCyZkoSJg2NL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Version normal con input"
      ],
      "metadata": {
        "id": "3fyYSE5_YOdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "import openai\n",
        "import argparse\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile,join\n",
        "import pyperclip\n",
        "from textwrap import fill\n",
        "args = input(\"pregunta:\")\n",
        "openai.api_key = \"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\"\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-002\",\n",
        "    prompt=f\"{args}\",\n",
        "    max_tokens=300,\n",
        "    temperature=0.3\n",
        ")\n",
        "respuesta = response[\"choices\"][0][\"text\"]\n",
        "print(respuesta)\n",
        "# Obtener la respuesta del primer resultado\n",
        "# Formatear el resultado en una cadena de LaTeX y agregarlo al contenido total\n",
        "# pyperclip.copy(contenido_step03)"
      ],
      "metadata": {
        "id": "sYJWHmwjYQ-g",
        "outputId": "c902fd17-1221-4f76-fe27-6643adc75de5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pregunta:hola\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-f9ce4079ea22>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pregunta:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"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\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m response = openai.Completion.create(\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text-davinci-002\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{args}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    764\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             )\n",
            "\u001b[0;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## formateador a cadena en latex"
      ],
      "metadata": {
        "id": "Btsfk1xbX2f3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valor = \"\\item \\\\textbf{}{}{}:{}\\n\".format(\"{\",entry,\"}\",respuesta)"
      ],
      "metadata": {
        "id": "cjw2aB6hX953"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Version input ¿que hace el siguiente codigo?"
      ],
      "metadata": {
        "id": "VNP4rkcaXR-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "import openai\n",
        "import argparse\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile,join\n",
        "import pyperclip\n",
        "from textwrap import fill\n",
        "args = input(\"directorio:\")\n",
        "openai.api_key = \"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\"\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-002\",\n",
        "    prompt=f\"Que hace el siguiente codigo:\\n {f.read()}\",\n",
        "    max_tokens=1024,\n",
        "    temperature=0.3\n",
        ")\n",
        "respuesta = response[\"choices\"][0][\"text\"]\n",
        "print(respuesta)\n",
        "# Obtener la respuesta del primer resultado\n",
        "# Formatear el resultado en una cadena de LaTeX y agregarlo al contenido total\n",
        "# pyperclip.copy(contenido_step03)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FLIQtoENXZKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Versión con portapapeles input"
      ],
      "metadata": {
        "id": "WBhqHDtYW1nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "import openai\n",
        "import argparse\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile,join\n",
        "import pyperclip\n",
        "from textwrap import fill\n",
        "args = input(\"directorio:\")\n",
        "openai.api_key = \"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\"\n",
        "# Inicialización de una cadena vacía para almacenar los resultados\n",
        "# variable global del contenido\n",
        "contenido_step01 = \"\"\n",
        "# validación de la variable de args\n",
        "if basepath:\n",
        "    print(\"entro a basepath\")\n",
        "    # Iteración en la lista de archivos de un directorio\n",
        "    for entry in listdir(basepath):\n",
        "        # Ignorar directorios y procesar solo archivos\n",
        "        if isfile(os.path.join(basepath, entry)):\n",
        "            # Abrir el archivo en modo lectura\n",
        "            with open(join(basepath, entry), 'r') as f:\n",
        "                print(entry)\n",
        "                # Obtener la respuesta de OpenAI con la descripción del código\n",
        "                response = openai.Completion.create(\n",
        "                    engine=\"text-davinci-002\",\n",
        "                    prompt=f\"Que hace el siguiente codigo:\\n {f.read()}\",\n",
        "                    max_tokens=1024,\n",
        "                    temperature=0.3\n",
        "                )\n",
        "                # Obtener la respuesta del primer resultado\n",
        "                respuesta = response[\"choices\"][0][\"text\"]\n",
        "                # Formatear el resultado en una cadena de LaTeX y agregarlo al contenido total\n",
        "                valor = \"\\item \\\\textbf{}{}{}:{}\\n\".format(\"{\",entry,\"}\",respuesta)\n",
        "                # Imprimir el resultado en la consola para verificar la salida\n",
        "                print(valor)\n",
        "                contenido_step01 += valor\n",
        "if portapapeles:\n",
        "    print(\"entro a portapapeles\")\n",
        "    # Leer el contenido del portapapeles\n",
        "    list_clipboard = pyperclip.paste().split(\"&\")\n",
        "    for response_stp00 in list_clipboard:\n",
        "        # Dividir el contenido del portapapeles en líneas y generar una respuesta única\n",
        "        response_stp01 = openai.Completion.create(\n",
        "            engine=\"text-davinci-002\",\n",
        "            prompt=f\"{response_stp00}\",\n",
        "            max_tokens=1024,\n",
        "            temperature=0.3\n",
        "        )\n",
        "        # Respuesta de chat gpt\n",
        "        response_stp02 = response_stp01[\"choices\"][0][\"text\"]\n",
        "        # conversion de la respuesta a formato latex\n",
        "        # response_stp03 = \"\\item {}\\n\".format(response_stp02)\n",
        "        print(response_stp00,\":\")\n",
        "        print(response_stp02)\n",
        "        contenido_step01 += \"\\item \\\\textbf {%s}:\\n\" % response_stp00\n",
        "        contenido_step01 += response_stp02\n",
        "# importamos una funcion que da formato al contenido de una cadena\n",
        "contenido_step02 = fill(contenido_step01,width=80)\n",
        "# Agregamos los espacios en los puntos\n",
        "contenido_step03 = contenido_step02.replace(\".\",\".\\n\")\n",
        "# Copiar el contenido generado en el portapapeles del sistema\n",
        "pyperclip.copy(contenido_step03)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9OmNcj3hWxhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Versión de argparse"
      ],
      "metadata": {
        "id": "HgY61rNNWyxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "import openai\n",
        "import argparse\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile,join\n",
        "import pyperclip\n",
        "from textwrap import fill\n",
        "# ARGPARSE STEP 01 Instancia del parser\n",
        "parser = argparse.ArgumentParser()\n",
        "# agregar argumento opcional -p para leer del portapapeles\n",
        "parser.add_argument(\"-p\", \"--portapapeles\", action=\"store_true\", help=\"Leer de portapapeles\")\n",
        "# nombre del directorio\n",
        "parser.add_argument(\"-d\", \"--directorio\", help=\"Especifica un directorio\")\n",
        "# leer los argumentos paso obligatorio\n",
        "# args = parser.parse_args()\n",
        "args = input(\"directorio:\")\n",
        "# Inicializar sesión de GPT\n",
        "# Inicialización de API key de OpenAI\n",
        "openai.api_key = \"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\"\n",
        "# Inicialización de una cadena vacía para almacenar los resultados\n",
        "# variable global del contenido\n",
        "contenido_step01 = \"\"\n",
        "# ARGPARSE STEP 04 - Obtención de la lista de archivos en el directorio especificado\n",
        "basepath = args.directorio\n",
        "portapapeles = args.portapapeles\n",
        "# validación de la variable de args\n",
        "if basepath:\n",
        "    print(\"entro a basepath\")\n",
        "    # Iteración en la lista de archivos de un directorio\n",
        "    for entry in listdir(basepath):\n",
        "        # Ignorar directorios y procesar solo archivos\n",
        "        if isfile(os.path.join(basepath, entry)):\n",
        "            # Abrir el archivo en modo lectura\n",
        "            with open(join(basepath, entry), 'r') as f:\n",
        "                print(entry)\n",
        "                # Obtener la respuesta de OpenAI con la descripción del código\n",
        "                response = openai.Completion.create(\n",
        "                    engine=\"text-davinci-002\",\n",
        "                    prompt=f\"Que hace el siguiente codigo:\\n {f.read()}\",\n",
        "                    max_tokens=1024,\n",
        "                    temperature=0.3\n",
        "                )\n",
        "                # Obtener la respuesta del primer resultado\n",
        "                respuesta = response[\"choices\"][0][\"text\"]\n",
        "                # Formatear el resultado en una cadena de LaTeX y agregarlo al contenido total\n",
        "                valor = \"\\item \\\\textbf{}{}{}:{}\\n\".format(\"{\",entry,\"}\",respuesta)\n",
        "                # Imprimir el resultado en la consola para verificar la salida\n",
        "                print(valor)\n",
        "                contenido_step01 += valor\n",
        "if portapapeles:\n",
        "    print(\"entro a portapapeles\")\n",
        "    # Leer el contenido del portapapeles\n",
        "    list_clipboard = pyperclip.paste().split(\"&\")\n",
        "    for response_stp00 in list_clipboard:\n",
        "        # Dividir el contenido del portapapeles en líneas y generar una respuesta única\n",
        "        response_stp01 = openai.Completion.create(\n",
        "            engine=\"text-davinci-002\",\n",
        "            prompt=f\"{response_stp00}\",\n",
        "            max_tokens=1024,\n",
        "            temperature=0.3\n",
        "        )\n",
        "        # Respuesta de chat gpt\n",
        "        response_stp02 = response_stp01[\"choices\"][0][\"text\"]\n",
        "        # conversion de la respuesta a formato latex\n",
        "        # response_stp03 = \"\\item {}\\n\".format(response_stp02)\n",
        "        print(response_stp00,\":\")\n",
        "        print(response_stp02)\n",
        "        contenido_step01 += \"\\item \\\\textbf {%s}:\\n\" % response_stp00\n",
        "        contenido_step01 += response_stp02\n",
        "# importamos una funcion que da formato al contenido de una cadena\n",
        "contenido_step02 = fill(contenido_step01,width=80)\n",
        "# Agregamos los espacios en los puntos\n",
        "contenido_step03 = contenido_step02.replace(\".\",\".\\n\")\n",
        "# Copiar el contenido generado en el portapapeles del sistema\n",
        "pyperclip.copy(contenido_step03)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xSWwgl3oLJTK",
        "outputId": "6a772936-c33e-4d1d-8afd-33a28b4227e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "directorio:.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d87a29747521>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mcontenido_step01\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# ARGPARSE STEP 04 - Obtención de la lista de archivos en el directorio especificado\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mbasepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirectorio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mportapapeles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mportapapeles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# validación de la variable de args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'directorio'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}